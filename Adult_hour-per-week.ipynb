{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import layers, optimizers, models\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import Image, SVG\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "# from skrvm import RVR\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pandas.read_csv(\"adult.data\", encoding=\"UTF-8\", header=None)\n",
    "test_data = pandas.read_csv(\"adult.test\", encoding=\"UTF-8\", header=None, skiprows=1) # skip 第一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[14] = test_data[14].str.replace('.', '') # 將 test_data class label 最後多餘的點去掉\n",
    "train_data = train_data.replace(' ?', np.nan) # 將 missing value 改成 NaN\n",
    "test_data = test_data.replace(' ?', np.nan) # 將 missing value 改成 NaN\n",
    "\n",
    "# \n",
    "# if instances with unknown values are removed (train=30162, test=15060)\n",
    "# \n",
    "train_data = train_data.dropna() # 去掉 missing value 的那筆資料，剩下 30162 筆\n",
    "test_data = test_data.dropna() # 去掉 missing value 的那筆資料，剩下 15060 筆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 12]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = train_data.columns.tolist() # 把train_data的columns轉換成list\n",
    "cols.insert(14, cols.pop(cols.index(12))) # 調換 12 跟 14 列\n",
    "\n",
    "cols = test_data.columns.tolist() # 把test_data的columns轉換成list\n",
    "cols.insert(14,cols.pop(cols.index(12))) # 調換 12 跟 14 列\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.loc[:,cols] # set剛剛重新定位的columns\n",
    "# train_data\n",
    "test_data = test_data.loc[:,cols] # set剛剛重新定義的columns\n",
    "# test_data\n",
    "train_data.columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14] # 對train_data的cloumns重新命名，讓它能夠恢復名稱順序\n",
    "# train_data\n",
    "test_data.columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14] # 對test_data的cloumns重新命名，讓它能夠恢復名稱順序\n",
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用 Label Encoding 把字串以數字取代\n",
    "def transform_label(full_data):\n",
    "    transform_needed = [False,\n",
    "           True,\n",
    "           False,\n",
    "           True,\n",
    "           False,\n",
    "           True,\n",
    "           True,\n",
    "           True,\n",
    "           True,\n",
    "           True,\n",
    "           False,\n",
    "           False,\n",
    "           True,\n",
    "           True,\n",
    "           False]\n",
    "\n",
    "\n",
    "    result = np.zeros(shape=(full_data.shape[0], full_data.shape[1]), dtype=np.float32)\n",
    "\n",
    "    for i in range(len(transform_needed)):\n",
    "        if transform_needed[i]:\n",
    "            tmp_data = full_data.iloc[:, i].tolist()\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(tmp_data)\n",
    "            result[:, i] = encoder.transform(tmp_data)\n",
    "        else:\n",
    "            result[:, i] = full_data.iloc[:, i].tolist()\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對 training data 和 testing data 進行 label encoding 編碼\n",
    "transformed_train_data = transform_label(train_data)\n",
    "transformed_test_data = transform_label(test_data)\n",
    "\n",
    "# Feature scaling : 將資料正規化\n",
    "scaler = MinMaxScaler().fit(transformed_train_data)\n",
    "\n",
    "transformed_train_data = scaler.transform(transformed_train_data)\n",
    "transformed_test_data = scaler.transform(transformed_test_data)\n",
    "\n",
    "# 拆分成輸入資料和答案\n",
    "x_train = transformed_train_data[:, :13]\n",
    "y_train = transformed_train_data[:, 14]\n",
    "\n",
    "x_test = transformed_test_data[:, :13]\n",
    "y_test = transformed_test_data[:, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30136985 0.8333334  0.04333771 ... 0.02174022 0.         0.95      ]\n",
      " [0.4520548  0.6666667  0.04727738 ... 0.         0.         0.95      ]\n",
      " [0.2876712  0.33333334 0.13724391 ... 0.         0.         0.95      ]\n",
      " ...\n",
      " [0.56164384 0.33333334 0.09391367 ... 0.         0.         0.95      ]\n",
      " [0.06849313 0.33333334 0.1276201  ... 0.         0.         0.95      ]\n",
      " [0.47945207 0.5        0.18638337 ... 0.1502415  0.         0.95      ]]\n",
      "[0.39795917 0.12244897 0.39795917 ... 0.39795917 0.19387755 0.39795917]\n",
      "[[0.10958904 0.33333334 0.1448282  ... 0.         0.         0.925     ]\n",
      " [0.2876712  0.33333334 0.05169837 ... 0.         0.         0.925     ]\n",
      " [0.15068492 0.16666667 0.21971181 ... 0.         0.         0.925     ]\n",
      " ...\n",
      " [0.2876712  0.33333334 0.24556744 ... 0.         0.         0.925     ]\n",
      " [0.36986297 0.33333334 0.04767168 ... 0.05455054 0.         0.925     ]\n",
      " [0.24657533 0.5        0.11447065 ... 0.         0.         0.925     ]]\n",
      "[0.39795917 0.5        0.39795917 ... 0.5        0.39795917 0.6020408 ]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(y_train)\n",
    "print(x_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20208 samples, validate on 9954 samples\n",
      "Epoch 1/100\n",
      "20208/20208 [==============================] - 1s 42us/step - loss: 0.0344 - val_loss: 0.0147\n",
      "Epoch 2/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0142 - val_loss: 0.0148\n",
      "Epoch 3/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0133 - val_loss: 0.0164\n",
      "Epoch 4/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0129 - val_loss: 0.0132\n",
      "Epoch 5/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0126 - val_loss: 0.0145\n",
      "Epoch 6/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0125 - val_loss: 0.0128\n",
      "Epoch 7/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0123 - val_loss: 0.0135\n",
      "Epoch 8/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0122 - val_loss: 0.0125\n",
      "Epoch 9/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0121 - val_loss: 0.0131\n",
      "Epoch 10/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0119 - val_loss: 0.0133\n",
      "Epoch 11/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0120 - val_loss: 0.0130\n",
      "Epoch 12/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0118 - val_loss: 0.0132\n",
      "Epoch 13/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0118 - val_loss: 0.0132\n",
      "Epoch 14/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0118 - val_loss: 0.0127\n",
      "Epoch 15/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0117 - val_loss: 0.0130\n",
      "Epoch 16/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0117 - val_loss: 0.0134\n",
      "Epoch 17/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0117 - val_loss: 0.0128\n",
      "Epoch 18/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0116 - val_loss: 0.0154\n",
      "Epoch 19/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0116 - val_loss: 0.0132\n",
      "Epoch 20/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0116 - val_loss: 0.0122\n",
      "Epoch 21/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0115 - val_loss: 0.0137\n",
      "Epoch 22/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0115 - val_loss: 0.0130\n",
      "Epoch 23/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0114 - val_loss: 0.0126\n",
      "Epoch 24/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0114 - val_loss: 0.0133\n",
      "Epoch 25/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0113 - val_loss: 0.0121\n",
      "Epoch 26/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0114 - val_loss: 0.0130\n",
      "Epoch 27/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0113 - val_loss: 0.0134\n",
      "Epoch 28/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0113 - val_loss: 0.0128\n",
      "Epoch 29/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0113 - val_loss: 0.0130\n",
      "Epoch 30/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0113 - val_loss: 0.0126\n",
      "Epoch 31/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0112 - val_loss: 0.0134\n",
      "Epoch 32/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0112 - val_loss: 0.0128\n",
      "Epoch 33/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 34/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0112 - val_loss: 0.0128\n",
      "Epoch 35/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0111 - val_loss: 0.0137\n",
      "Epoch 36/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 37/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0112 - val_loss: 0.0151\n",
      "Epoch 38/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0111 - val_loss: 0.0128\n",
      "Epoch 39/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0111 - val_loss: 0.0132\n",
      "Epoch 40/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0111 - val_loss: 0.0128\n",
      "Epoch 41/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0110 - val_loss: 0.0125\n",
      "Epoch 42/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0110 - val_loss: 0.0127\n",
      "Epoch 43/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 44/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0110 - val_loss: 0.0132\n",
      "Epoch 45/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 46/100\n",
      "20208/20208 [==============================] - 1s 35us/step - loss: 0.0110 - val_loss: 0.0124\n",
      "Epoch 47/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 48/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 49/100\n",
      "20208/20208 [==============================] - 1s 35us/step - loss: 0.0109 - val_loss: 0.0128\n",
      "Epoch 50/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0109 - val_loss: 0.0140\n",
      "Epoch 51/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0109 - val_loss: 0.0128\n",
      "Epoch 52/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 53/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 54/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0109 - val_loss: 0.0124\n",
      "Epoch 55/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0109 - val_loss: 0.0127\n",
      "Epoch 56/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0109 - val_loss: 0.0133\n",
      "Epoch 57/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 58/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0108 - val_loss: 0.0124\n",
      "Epoch 59/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 60/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 61/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0107 - val_loss: 0.0126\n",
      "Epoch 62/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 63/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 64/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 65/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0107 - val_loss: 0.0127\n",
      "Epoch 66/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0107 - val_loss: 0.0126\n",
      "Epoch 67/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0107 - val_loss: 0.0136\n",
      "Epoch 68/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 69/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 70/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 71/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0107 - val_loss: 0.0128\n",
      "Epoch 72/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 73/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 74/100\n",
      "20208/20208 [==============================] - 1s 35us/step - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 75/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 76/100\n",
      "20208/20208 [==============================] - 1s 34us/step - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 77/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 78/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 79/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 80/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 81/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0105 - val_loss: 0.0123\n",
      "Epoch 82/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0105 - val_loss: 0.0124\n",
      "Epoch 83/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 84/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0105 - val_loss: 0.0123\n",
      "Epoch 85/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 86/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 87/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 88/100\n",
      "20208/20208 [==============================] - 1s 33us/step - loss: 0.0105 - val_loss: 0.0119\n",
      "Epoch 89/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 90/100\n",
      "20208/20208 [==============================] - 1s 32us/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 91/100\n",
      "20208/20208 [==============================] - 1s 29us/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 92/100\n",
      "20208/20208 [==============================] - 1s 26us/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 93/100\n",
      "20208/20208 [==============================] - 0s 24us/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 94/100\n",
      "20208/20208 [==============================] - 1s 31us/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 95/100\n",
      "20208/20208 [==============================] - 1s 30us/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 96/100\n",
      "20208/20208 [==============================] - 1s 30us/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 97/100\n",
      "20208/20208 [==============================] - 1s 30us/step - loss: 0.0104 - val_loss: 0.0129\n",
      "Epoch 98/100\n",
      "20208/20208 [==============================] - 1s 31us/step - loss: 0.0104 - val_loss: 0.0124\n",
      "Epoch 99/100\n",
      "20208/20208 [==============================] - 1s 31us/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 100/100\n",
      "20208/20208 [==============================] - 1s 31us/step - loss: 0.0105 - val_loss: 0.0121\n"
     ]
    }
   ],
   "source": [
    "# 建構 model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, input_shape=(x_train.shape[1],), activation=\"relu\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.15))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, kernel_initializer=\"uniform\",activation='linear'))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# 開始訓練 model\n",
    "history = model.fit(x_train, y_train, validation_split=0.33, epochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_66 (Dense)             (None, 64)                896       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,577\n",
      "Trainable params: 4,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 顯示模型摘要與結構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15060, 1)\n",
      "(15060, 1)\n",
      "RMSE: 10.766541\n",
      "MAPE: 25.803733\n"
     ]
    }
   ],
   "source": [
    "# 評估指標\n",
    "\n",
    "y_test = test_data[14].to_numpy().reshape(len(test_data[14].to_numpy()), 1) # 取得答案\n",
    "print(y_test.shape)\n",
    "scaler = MinMaxScaler().fit(y_test) # 將答案的值進行正規化\n",
    "\n",
    "yhat = model.predict(x_test)\n",
    "print(yhat.shape)\n",
    "preds = scaler.inverse_transform(yhat)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "mape = np.mean(np.abs((preds - y_test) / y_test)) * 100\n",
    "print(\"MAPE: %f\" % (mape))\n",
    "\n",
    "# 畫圖\n",
    "# plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n",
    "# plt.plot(history.history['loss'], label='loss')\n",
    "# plt.plot(history.history['val_loss'], label='val_loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對 training data 和 testing data 進行 label encoding 編碼\n",
    "transformed_train_data = transform_label(train_data)\n",
    "transformed_test_data = transform_label(test_data)\n",
    "\n",
    "# Feature scaling : 將資料正規化\n",
    "scaler = MinMaxScaler().fit(transformed_train_data)\n",
    "\n",
    "transformed_train_data = scaler.transform(transformed_train_data)\n",
    "transformed_test_data = scaler.transform(transformed_test_data)\n",
    "\n",
    "# 拆分成輸入資料和答案\n",
    "x_train = transformed_train_data[:, :13]\n",
    "y_train = transformed_train_data[:, 14]\n",
    "\n",
    "x_test = transformed_test_data[:, :13]\n",
    "y_test = transformed_test_data[:, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(epsilon=0.001)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr = SVR(gamma='scale',epsilon=0.001, )\n",
    "# svr = SVR(kernel='rbf', gamma=scale, epsilon=0.001)\n",
    "# RMSE: 10.966594\n",
    "# MAPE: 26.509665\n",
    "\n",
    "svr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15060, 1)\n",
      "(15060,)\n",
      "(15060, 1)\n",
      "RMSE: 10.966594\n",
      "MAPE: 26.509665\n"
     ]
    }
   ],
   "source": [
    "# 評估指標\n",
    "\n",
    "y_test = test_data[14].to_numpy().reshape(len(test_data[14].to_numpy()), 1) # 取得答案\n",
    "print(y_test.shape)\n",
    "scaler = MinMaxScaler().fit(y_test) # 將答案的值進行正規化\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "yhat = svr.predict(x_test)\n",
    "print(yhat.shape)\n",
    "preds = scaler.inverse_transform(yhat.reshape(-1, 1))\n",
    "print(preds.shape)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "mape = np.mean(np.abs((preds - y_test) / y_test)) * 100\n",
    "print(\"MAPE: %f\" % (mape))\n",
    "\n",
    "# 畫圖\n",
    "# plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n",
    "# plt.plot(history.history['loss'], label='loss')\n",
    "# plt.plot(history.history['val_loss'], label='val_loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對 training data 和 testing data 進行 label encoding 編碼\n",
    "transformed_train_data = transform_label(train_data)\n",
    "transformed_test_data = transform_label(test_data)\n",
    "\n",
    "# Feature scaling : 將資料正規化\n",
    "scaler = MinMaxScaler().fit(transformed_train_data)\n",
    "\n",
    "transformed_train_data = scaler.transform(transformed_train_data)\n",
    "transformed_test_data = scaler.transform(transformed_test_data)\n",
    "\n",
    "# 拆分成輸入資料和答案\n",
    "x_train = transformed_train_data[:, :13]\n",
    "y_train = transformed_train_data[:, 14]\n",
    "\n",
    "x_test = transformed_test_data[:, :13]\n",
    "y_test = transformed_test_data[:, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, n_estimators=1450, random_state=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = RandomForestRegressor(max_depth=10, random_state=0,n_estimators = 1450)\n",
    "RF.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15060, 1)\n",
      "(15060,)\n",
      "(15060, 1)\n",
      "RMSE: 10.466266\n",
      "MAPE: 26.201979\n"
     ]
    }
   ],
   "source": [
    "# 評估指標\n",
    "\n",
    "y_test = test_data[14].to_numpy().reshape(len(test_data[14].to_numpy()), 1) # 取得答案\n",
    "print(y_test.shape)\n",
    "scaler = MinMaxScaler().fit(y_test) # 將答案的值進行正規化\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "yhat = RF.predict(x_test)\n",
    "print(yhat.shape)\n",
    "preds = scaler.inverse_transform(yhat.reshape(-1, 1))\n",
    "print(preds.shape)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "mape = np.mean(np.abs((preds - y_test) / y_test)) * 100\n",
    "print(\"MAPE: %f\" % (mape))\n",
    "\n",
    "# 畫圖\n",
    "# plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n",
    "# plt.plot(history.history['loss'], label='loss')\n",
    "# plt.plot(history.history['val_loss'], label='val_loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對 training data 和 testing data 進行 label encoding 編碼\n",
    "transformed_train_data = transform_label(train_data)\n",
    "transformed_test_data = transform_label(test_data)\n",
    "\n",
    "# Feature scaling : 將資料正規化\n",
    "scaler = MinMaxScaler().fit(transformed_train_data)\n",
    "\n",
    "transformed_train_data = scaler.transform(transformed_train_data)\n",
    "transformed_test_data = scaler.transform(transformed_test_data)\n",
    "\n",
    "# 拆分成輸入資料和答案\n",
    "x_train = transformed_train_data[:, :13]\n",
    "y_train = transformed_train_data[:, 14]\n",
    "\n",
    "x_test = transformed_test_data[:, :13]\n",
    "y_test = transformed_test_data[:, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IDSL\\anaconda3\\lib\\site-packages\\xgboost\\core.py:444: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=105, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=0.0011, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', booster = 'gbtree',\n",
    "                              reg_lambda = 0.0011, learning_rate = 0.1, n_estimators = 105)\n",
    "\n",
    "xg_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.389408\n",
      "MAPE: 25.760196\n"
     ]
    }
   ],
   "source": [
    "# 評估指標\n",
    "\n",
    "y_test = test_data[14].to_numpy().reshape(len(test_data[14].to_numpy()), 1) # 取得答案\n",
    "# print(y_test.shape)\n",
    "scaler = MinMaxScaler().fit(y_test) # 將答案的值進行正規化\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "yhat = xg_reg.predict(x_test)\n",
    "# print(yhat.shape)\n",
    "preds = scaler.inverse_transform(yhat.reshape(-1, 1))\n",
    "# print(preds.shape)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "mape = np.mean(np.abs((preds - y_test) / y_test)) * 100\n",
    "print(\"MAPE: %f\" % (mape))\n",
    "\n",
    "# 畫圖\n",
    "# plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n",
    "# plt.plot(history.history['loss'], label='loss')\n",
    "# plt.plot(history.history['val_loss'], label='val_loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
